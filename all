from azure.identity import DefaultAzureCredential
from azure.storage.filedatalake import DataLakeServiceClient
import os

# Your existing setup
DATA_LAKE_ACCOUNT = os.getenv('DATA_LAKE_ACCOUNT', 'redprdprivatelake')
DATA_LAKE_FILESYSTEM = os.getenv('DATA_LAKE_FILESYSTEM', 'regression')

# Using DefaultAzureCredential (same as your existing code)
credential = DefaultAzureCredential()
service_client = DataLakeServiceClient(
    f"https://{DATA_LAKE_ACCOUNT}.dfs.core.windows.net",
    credential=credential
)


def list_directory(directory_path: str = "") -> list[dict]:
    """
    List all files in a directory.
    """
    filesystem_client = service_client.get_file_system_client(DATA_LAKE_FILESYSTEM)
    paths = filesystem_client.get_paths(path=directory_path)
    
    file_list = []
    for path in paths:
        file_list.append({
            'name': path.name,
            'is_directory': path.is_directory,
            'size': path.content_length,
            'last_modified': path.last_modified
        })
    return file_list


def download_file(remote_path: str, local_path: str) -> str:
    """
    Download a single file from Data Lake.
    """
    filesystem_client = service_client.get_file_system_client(DATA_LAKE_FILESYSTEM)
    file_client = filesystem_client.get_file_client(remote_path)
    
    # Create local directory if needed
    local_dir = os.path.dirname(local_path)
    if local_dir:
        os.makedirs(local_dir, exist_ok=True)
    
    # Download file
    with open(local_path, 'wb') as f:
        download = file_client.download_file()
        f.write(download.readall())
    
    return local_path


def download_directory(remote_directory: str, local_directory: str) -> list[str]:
    """
    Download all files from a Data Lake directory.
    """
    filesystem_client = service_client.get_file_system_client(DATA_LAKE_FILESYSTEM)
    paths = filesystem_client.get_paths(path=remote_directory, recursive=True)
    
    downloaded_files = []
    
    for path in paths:
        if not path.is_directory:
            # Calculate relative path
            relative_path = path.name[len(remote_directory):].lstrip('/')
            local_file_path = os.path.join(local_directory, relative_path)
            
            # Download
            download_file(path.name, local_file_path)
            downloaded_files.append(local_file_path)
            print(f"Downloaded: {path.name} -> {local_file_path}")
    
    return downloaded_files


# If you need to use SAS token instead of DefaultAzureCredential
def get_client_with_sas(account_name: str, sas_token: str):
    """
    Create client using SAS token instead of DefaultAzureCredential.
    """
    if not sas_token.startswith('?'):
        sas_token = '?' + sas_token
    
    account_url = f"https://{account_name}.dfs.core.windows.net{sas_token}"
    return DataLakeServiceClient(account_url=account_url)


# Example usage
if __name__ == "__main__":
    # List files in a directory
    files = list_directory("your/directory/path")
    for f in files:
        print(f"{f['name']} - {'DIR' if f['is_directory'] else f['size']} bytes")
    
    # Download entire directory
    downloaded = download_directory(
        remote_directory="your/source/directory",
        local_directory="./downloaded_data"
    )
    print(f"Downloaded {len(downloaded)} files")
